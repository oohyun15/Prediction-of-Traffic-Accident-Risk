{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1\n",
    "# Prediction of Traffic Accident Risk\n",
    "Team 7: 어서오십쇼HUMAN  \n",
    "Editor: 유성민, 김도운"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2\n",
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        사고내용  도로형태 대분류  기상상태       차량년식     가해자성별     가해자나이  요일      가해차종  \\\n",
      "28153      3       0.0     1   6.000000  2.000000  3.178054   2  9.000000   \n",
      "70800      3       0.0     1   3.000000  1.000000  4.158883   7  9.000000   \n",
      "103699     1       0.0     1   2.152682  1.000000  3.036882   7  3.000000   \n",
      "98444      1       0.0     1  14.254473  1.000000  3.441304   6  9.254473   \n",
      "18541      3       0.0     1   7.000000  2.000000  3.555348   6  9.000000   \n",
      "...      ...       ...   ...        ...       ...       ...  ..       ...   \n",
      "4323       2       0.0     1   1.000000  1.000000  3.178054   1  3.000000   \n",
      "160243     2       0.0     2   2.506576  1.493424  4.191375   1  9.000000   \n",
      "121255     1       0.0     1  11.647076  1.000000  4.059710   1  4.117641   \n",
      "33981      3       0.0     2   6.000000  1.000000  3.912023   2  9.000000   \n",
      "96946      1       0.0     1   1.122018  2.000000  3.796391   4  9.000000   \n",
      "\n",
      "            노면상태    보행자이동  빛 세기     사고 위치     사고 조절  \n",
      "28153   1.000000  0.00000     6  1.000000  4.000000  \n",
      "70800   1.000000  0.00000     1  2.000000  4.000000  \n",
      "103699  1.000000  0.00000     1  1.923659  4.000000  \n",
      "98444   1.000000  0.38171     1  1.127237  4.000000  \n",
      "18541   1.000000  0.00000     4  8.000000  4.000000  \n",
      "...          ...      ...   ...       ...       ...  \n",
      "4323    2.000000  0.00000     1  8.000000  4.000000  \n",
      "160243  2.000000  0.00000     4  4.986848  4.000000  \n",
      "121255  1.000000  0.00000     4  1.000000  4.000000  \n",
      "33981   2.000000  0.00000     1  2.000000  4.000000  \n",
      "96946   1.877982  0.00000     1  1.000000  3.122018  \n",
      "\n",
      "[189170 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "preprocessing = pd.read_csv('../input/final1.csv')\n",
    "\n",
    "preprocessing = preprocessing.iloc[np.random.permutation(len(preprocessing))]\n",
    "\n",
    "y = preprocessing.loc[:, preprocessing.columns == '사고내용']\n",
    "x = preprocessing.loc[:, preprocessing.columns != '사고내용']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "print(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5\n",
    "# Do you need standardization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//6\n",
       "var input = prompt('Do you need standardization? then if you want input y', 'waiting your input ......');\n",
       "var i;\n",
       "if(input == 'y'){\n",
       "     alert('표준화를 진행합니다');\n",
       "    Jupyter.notebook.execute_cells([3]); // read data\n",
       "     for(i=35; i<=61; i++) {\n",
       "        Jupyter.notebook.execute_cells([i]); // learning\n",
       "     }\n",
       "}else{\n",
       "    alert('표준화 없이 진행합니다.');\n",
       "      Jupyter.notebook.execute_cells([3]); // read data\n",
       "     for(i=7; i<=34; i++) {\n",
       "        Jupyter.notebook.execute_cells([i]); // learning\n",
       "     }\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//6\n",
    "var input = prompt('Do you need standardization? then if you want input y', 'waiting your input ......');\n",
    "var i;\n",
    "if(input == 'y'){\n",
    "     alert('표준화를 진행합니다');\n",
    "    Jupyter.notebook.execute_cells([3]); // read data\n",
    "     for(i=35; i<=61; i++) {\n",
    "        Jupyter.notebook.execute_cells([i]); // learning\n",
    "     }\n",
    "}else{\n",
    "    alert('표준화 없이 진행합니다.');\n",
    "      Jupyter.notebook.execute_cells([3]); // read data\n",
    "     for(i=7; i<=34; i++) {\n",
    "        Jupyter.notebook.execute_cells([i]); // learning\n",
    "     }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7\n",
    "## 2. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#8\n",
    "# Prepare Model training and analysis\n",
    "\n",
    "> editor: seongminyoo   \n",
    "> Date of modification: 2020-05-25 04:49pm\n",
    "\n",
    "`solving problem`\n",
    "+ Classification Problem\n",
    "\n",
    "`using model`\n",
    "+ KNN\n",
    "+ Decision Tree\n",
    "+ Bagging\n",
    "+ Random Forest\n",
    "+ SVM\n",
    "+ Neural Network classifier\n",
    "\n",
    "`algorithm for each step`\n",
    "1. GridSearchCV by 10 fold or 5 fold\n",
    "    - make parameter set\n",
    "2. Test\n",
    "    - best_esitimater is model that construct best hyper parameter set\n",
    "3. Analysis result\n",
    "    - use confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#9\n",
    "# Make functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10\n",
    "# 최고 모델 만드는 함수\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def make_model(model, param_grid, cv):\n",
    "    try: # random_state 가능 모델\n",
    "        gs = GridSearchCV(estimator=model(random_state = 20000), param_grid=param_grid, \n",
    "                          scoring='balanced_accuracy', cv=cv)\n",
    "    except Exception: # random_state 불가능 모델\n",
    "        print('error detection!')\n",
    "        gs = GridSearchCV(estimator=model(), param_grid=param_grid, \n",
    "                          scoring='balanced_accuracy', cv=cv)\n",
    "    finally:\n",
    "        gs.fit(x_train,y_train)\n",
    "        best_params = gs.best_params_\n",
    "        print(best_params)\n",
    "        return gs\n",
    "\n",
    "# test function\n",
    "def make_test(model):\n",
    "        best_model=model.best_estimator_\n",
    "        y_pred = best_model.predict(x_test)\n",
    "        print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))\n",
    "        cm = confusion_matrix(y_test,y_pred)\n",
    "        print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 50개의 Thread를 생성하여 병렬처리 -시간 매우 단축-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11\n",
    "from concurrent.futures import as_completed, ProcessPoolExecutor\n",
    "import time\n",
    "import numpy as np\n",
    "import winprocess\n",
    "\n",
    "def parallel_processing(model, param_grid, cv):\n",
    "    executor = ProcessPoolExecutor(max_workers=50)\n",
    "    fs = winprocess.submit(executor, make_model,model, param_grid, cv)\n",
    "    return fs.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#12\n",
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(3,10)]\n",
    "param_grid = [\n",
    "    {'n_neighbors': param_range, 'metric': ['euclidean']},\n",
    "    {'n_neighbors': param_range, 'metric': ['manhattan']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "best_model = parallel_processing(KNeighborsClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.98      1.00      0.99     12512\n",
      "     class 1       0.89      0.94      0.91     12703\n",
      "     class 2       0.94      0.86      0.90     12619\n",
      "\n",
      "    accuracy                           0.93     37834\n",
      "   macro avg       0.93      0.93      0.93     37834\n",
      "weighted avg       0.93      0.93      0.93     37834\n",
      "\n",
      "Confusion matrix: \n",
      " [[12476    15    21]\n",
      " [   85 11943   675]\n",
      " [  195  1521 10903]]\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#16\n",
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#17\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(3,7)]\n",
    "# 1 아래의 parmeter들과 위의 k가 한번 씩 모두 들어가서 brute-force를 진행함으로써 제일 좋은 파라미터를  결과로 도출해준다.\n",
    "param_grid = [\n",
    "    {'max_depth': param_range, 'criterion': ['entropy']},\n",
    "    {'max_depth': param_range, 'criterion': ['gini']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#18\n",
    "# Decision Tree train and test\n",
    "best_model = parallel_processing(tree.DecisionTreeClassifier,param_grid,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#19\n",
    "make_test(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#20\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#21\n",
    "# hyper parameter set for Random Forest\n",
    "estimators = [100,200]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'max_depth': depth_range, 'criterion': ['entropy'], 'n_estimators':estimators},\n",
    "    {'max_depth': depth_range, 'criterion': ['gini'], 'n_estimators':estimators}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22\n",
    "# Random Forest train and test\n",
    "best_model = parallel_processing(RandomForestClassifier,param_grid,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#23\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#24\n",
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25\n",
    "# hyper parameter set for Bagging\n",
    "estimators = [100, 200]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'base_estimator__max_depth': depth_range, 'base_estimator__criterion': ['entropy'], 'n_estimators':estimators},\n",
    "    {'base_estimator__max_depth': depth_range, 'base_estimator__criterion': ['gini'], 'n_estimators':estimators}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#26\n",
    "#Bagging train and test\n",
    "\n",
    "gs = GridSearchCV(estimator=BaggingClassifier(tree.DecisionTreeClassifier(random_state = 1),random_state=20000)\n",
    "                  , param_grid=param_grid, scoring='balanced_accuracy', cv=10)\n",
    "gs.fit(x_train,y_train)\n",
    "print(gs.best_params_)\n",
    "best_bg=gs.best_estimator_\n",
    "y_pred = best_bg.predict(x_test) \n",
    "print(classification_report(y_test,y_pred,target_names=['class 1','class 2','class 3']))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#27\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#28\n",
    "# hyper parameter set for SVM\n",
    "param_range = [0.01, 1]\n",
    "param_grid = [\n",
    "    {'C': param_range, 'kernel': ['linear']},\n",
    "    {'C': param_range, 'kernel': ['rbf']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29\n",
    "# SVM Forest train and test\n",
    "best_model = parallel_processing(SVC,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#31\n",
    "# Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#32\n",
    "# hyper parameter set for Neural Network\n",
    "param_learning_rate=['constant', 'adaptive']\n",
    "param_solver=['sgd', 'adam']\n",
    "param_hidden_layer_sizes=[(100,)]\n",
    "param_grid = [\n",
    "    {'learning_rate': param_learning_rate, 'hidden_layer_sizes':param_hidden_layer_sizes,'solver':param_solver}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#33\n",
    "# Neural Network train and test\n",
    "best_model = parallel_processing(MLPClassifier,param_grid, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#34\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class imbalance에 적합한 모델 선정 중 -테스트-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(x_train,y_train)\n",
    "\n",
    "y_pred  = logreg.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier()\n",
    "sgd.fit(x_train, y_train)\n",
    "y_pred = sgd.predict(x_test)\n",
    "print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.00      0.00      0.00       528\n",
      "     class 1       0.17      0.07      0.10      5026\n",
      "     class 2       0.85      0.95      0.90     31168\n",
      "\n",
      "    accuracy                           0.81     36722\n",
      "   macro avg       0.34      0.34      0.33     36722\n",
      "weighted avg       0.75      0.81      0.77     36722\n",
      "\n",
      "Confusion matrix: \n",
      " [[    0    28   500]\n",
      " [    5   353  4668]\n",
      " [   36  1668 29464]]\n"
     ]
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(y_train, y_train)\n",
    "y_pred = gaussian.predict(x_test)\n",
    "print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.00      0.00      0.00        29\n",
      "     class 1       0.00      0.00      0.00       292\n",
      "     class 2       0.84      1.00      0.91      1679\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.28      0.33      0.30      2000\n",
      "weighted avg       0.70      0.84      0.77      2000\n",
      "\n",
      "Confusion matrix: \n",
      " [[   0    0   29]\n",
      " [   0    0  292]\n",
      " [   0    0 1679]]\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(x_train, y_train)\n",
    "y_pred = perceptron.predict(x_test)\n",
    "print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.00      0.00      0.00        29\n",
      "     class 1       0.00      0.00      0.00       292\n",
      "     class 2       0.84      1.00      0.91      1679\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.28      0.33      0.30      2000\n",
      "weighted avg       0.70      0.84      0.77      2000\n",
      "\n",
      "Confusion matrix: \n",
      " [[   0    0   29]\n",
      " [   0    0  292]\n",
      " [   0    0 1679]]\n"
     ]
    }
   ],
   "source": [
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(x_train, y_train)\n",
    "y_pred = linear_svc.predict(x_test)\n",
    "print(classification_report(y_test,y_pred,target_names=['class 0','class 1','class 2']))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standard scaler pipeline적용된 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#35\n",
    "# Prepare Model training and analysis\n",
    "\n",
    "> editor: seongminyoo   \n",
    "> Date of modification: 2020-05-25 04:49pm\n",
    "\n",
    "`solving problem`\n",
    "+ Classification Problem\n",
    "\n",
    "`using model`\n",
    "+ KNN\n",
    "+ Decision Tree\n",
    "+ Bagging\n",
    "+ Random Forest\n",
    "+ SVM\n",
    "+ Neural Network classifier\n",
    "\n",
    "`algorithm for each step`\n",
    "1. Make pipeline\n",
    "    - standard data, model\n",
    "2. GridSearchCV by 10 fold or 5 fold\n",
    "    - make parameter set\n",
    "3. Test\n",
    "    - best_esitimater is model that construct best hyper parameter set\n",
    "4. Analysis result\n",
    "    - use confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#36\n",
    "# Make functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#37\n",
    "def make_model(model_name, model, param_grid, cv):\n",
    "    try: # random_state 가능 모델\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), (model_name,model(random_state = 1))])\n",
    "    except Exception: # random_state 불가능 모델\n",
    "        print('error detection!')\n",
    "        pipe_svc = Pipeline([('scl', StandardScaler()), (model_name,model())])\n",
    "    finally:\n",
    "        gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=cv)\n",
    "        gs.fit(x_train,y_train)\n",
    "        best_params = gs.best_params_\n",
    "        print(best_params)\n",
    "        return gs\n",
    "\n",
    "# test function\n",
    "def make_test(model):\n",
    "        best_model=model.best_estimator_\n",
    "        y_pred = best_model.predict(x_test)\n",
    "        print(classification_report(y_test,y_pred,target_names=['class 1','class 2','class 3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#38\n",
    "from concurrent.futures import as_completed, ProcessPoolExecutor\n",
    "import time\n",
    "import numpy as np\n",
    "import winprocess\n",
    "\n",
    "def parallel_processing(model_name, model, param_grid, cv):\n",
    "    executor = ProcessPoolExecutor(max_workers=50)\n",
    "    fs = winprocess.submit(executor, make_model, model_name, model, param_grid, cv)\n",
    "    return fs.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#39\n",
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#40\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(1,11)]\n",
    "param_grid = [\n",
    "    {'knn__n_neighbors': param_range, 'knn__metric': ['euclidean']},\n",
    "    {'knn__n_neighbors': param_range, 'knn__metric': ['manhattan']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#41\n",
    "best_model = parallel_processing('knn',KNeighborsClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#42\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#43\n",
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#44\n",
    "# hyper parameter sets\n",
    "param_range = [i for i in range(3,7)]\n",
    "# 1 아래의 parmeter들과 위의 k가 한번 씩 모두 들어가서 brute-force를 진행함으로써 제일 좋은 파라미터를  결과로 도출해준다.\n",
    "param_grid = [\n",
    "    {'dt__max_depth': param_range, 'dt__criterion': ['entropy']},\n",
    "    {'dt__max_depth': param_range, 'dt__criterion': ['gini']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#45\n",
    "# Decision Tree train and test\n",
    "best_model = parallel_processing('dt',tree.DecisionTreeClassifier,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#46\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#47\n",
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#48\n",
    "# hyper parameter set for Random Forest\n",
    "estimators = [10,100]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'rf__max_depth': depth_range, 'rf__criterion': ['entropy'], 'rf__n_estimators':estimators},\n",
    "    {'rf__max_depth': depth_range, 'rf__criterion': ['gini'], 'rf__n_estimators':estimators}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#49\n",
    "# Random Forest train and test\n",
    "best_model = parallel_processing('rf',RandomForestClassifier,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#51\n",
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#52\n",
    "# hyper parameter set for Bagging\n",
    "estimators = [10, 100]\n",
    "depth_range = [i for i in range(3,7)]\n",
    "param_grid = [\n",
    "    {'bg__base_estimator__max_depth': depth_range, 'bg__base_estimator__criterion': ['entropy'], 'bg__n_estimators':estimators},\n",
    "    {'bg__base_estimator__max_depth': depth_range, 'bg__base_estimator__criterion': ['gini'], 'bg__n_estimators':estimators}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#53\n",
    "#Bagging train and test\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()), ('bg',BaggingClassifier(tree.DecisionTreeClassifier(random_state = 1),random_state=1))])\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=5,)\n",
    "gs.fit(x_train,y_train)\n",
    "print(gs.best_params_)\n",
    "best_bg=gs.best_estimator_\n",
    "y_pred = best_bg.predict(x_test) \n",
    "print(classification_report(y_test,y_pred,target_names=['lsclass 1','class 2','class 3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#54\n",
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#55\n",
    "# hyper parameter set for SVM\n",
    "param_range = [0.01, 1 ,10.0]\n",
    "param_grid = [\n",
    "    {'clf__C': param_range, 'clf__kernel': ['linear']},\n",
    "    {'clf__C': param_range, 'clf__kernel': ['rbf']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#56\n",
    "# SVM Forest train and test\n",
    "best_model = parallel_processing('clf',SVC,param_grid,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#57\n",
    "make_test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#58\n",
    "# Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#59\n",
    "# hyper parameter set for Neural Network\n",
    "param_learning_rate=['constant', 'adaptive']\n",
    "param_activation = ['relu', 'identity', 'tanh', 'logistic']\n",
    "param_solver=['sgd', 'adam']\n",
    "param_hidden_layer_sizes=[(100,)]\n",
    "param_grid = [\n",
    "    {'nnc__learning_rate': param_learning_rate, 'nnc__hidden_layer_sizes':param_hidden_layer_sizes, \n",
    "     'nnc__activation':param_activation, 'nnc__solver':param_solver}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60\n",
    "# Neural Network train and test\n",
    "best_model = parallel_processing('nnc',MLPClassifier,param_grid, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#61\n",
    "make_test(best_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
